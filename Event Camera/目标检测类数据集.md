# 事件相机目标检测类数据集

[TOC]

#### [NeurIPS 2020](https://paperswithcode.com/conference/neurips-2020-12) Learning to Detect Objects with a 1 Megapixel Event Camera

- <https://paperswithcode.com/paper/learning-to-detect-objects-with-a-1-megapixel>

- <https://www.prophesee.ai/category/dataset/>

​	在本文中，首先，我们公开发布第一个高分辨率大规模数据集以进行对象检测。该数据集包含1兆像素事件摄像头的14小时以上的录音，以及25m框架的汽车，行人和两轮车，并以高频标记。

#### CVPRW 2023 PEDRo: an Event-based Dataset for Person Detection in Robotics

- ​	<https://github.com/SSIGPRO/PEDRo-Event-Based-Dataset>


​	PEDRO事件数据集是专门为服务机器人技术检测而设计的。该数据集是通过在各种场景和照明条件下使用移动的Davis346活动摄像头收集的。

​	该数据集由：

- 119张记录，平均持续时间为18秒。
- 43 259手动注释的边界框。
- 27000个40毫秒的样品。

​	该数据集专注于人员，使其与其他基于事件的数据集成为处理人检测任务的相关补充。PEDRO数据集可以在此处下载。

#### EventVOT_Benchmark Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric

- EventVOT_Benchmark：<https://github.com/Event-AHU/EventVOT_Benchmark>
- <https://github.com/Event-AHU/COESOT>	

​	结合色彩和事件相机（又称动态视觉传感器，DVS）进行鲁棒性物体跟踪是近年来新出现的研究课题。现有的颜色-事件跟踪框架通常包含多个分散的模块，可能导致效率低、计算复杂度高，包括特征提取、融合、匹配、交互式学习等。本文提出了一种单级骨干网络色彩事件统一跟踪（CEUTrack），可同时实现上述功能。给定事件点和 RGB 帧后，我们首先将点转换为体素，并分别裁剪出两种模式的模板区域和搜索区域。然后，将这些区域投影成标记，并平行输入统一的 Transformer 骨干网络。输出的特征将输入跟踪头，用于目标对象定位。我们提出的 CEUTrack 简单、有效、高效，可实现超过 75 FPS 的速度和全新的 SOTA 性能。为了更好地验证我们模型的有效性并解决该任务数据不足的问题，我们还提出了一个通用的大规模色彩事件跟踪基准数据集，称为 COESOT，其中包含 90 个类别和 1354 个视频序列。此外，我们还在评估工具包中提出了一个名为 BOC 的新评估指标，用于评估相对于基准方法的突出性。我们希望新提出的方法、数据集和评价指标能为基于颜色事件的跟踪提供一个更好的平台。

- ​	Demo video for EventVOT dataset <https://www.youtube.com/watch?v=FcwH7tkSXK0>


#### Frontiers in neurorobotics 2019 Neuromorphic Vision Datasets for Pedestrian Detection, Action Recognition, and Fall Detection

- ​	<https://github.com/CrystalMiaoshu/PAFBenchmark>


​	通过SAE编码方法，将大部分行人检测原始数据转换为4670帧图像，帧间隔为20ms。在我们的实验中，所有这些图像都通过标注工具labelImg进行了标注。

#### CVPRW 2021 DVS-OUTLAB: A Neuromorphic Event-Based Long Time Monitoring Dataset for Real-World Outdoor Scenarios

​	神经形态视觉传感器是具有生物学启发的设备，与众所周知的基于框架的传感器有不同。尽管该研究领域的发展正在增加，但完全依赖事件摄像机的应用程序仍然相对罕见。除了实验室条件之外，考虑到实际室外场景时，这一点尤其清楚。在这种情况下，基于事件的视力应用程序开发的一个障碍可能是缺乏用于算法开发和评估的标签数据集。因此，我们描述了基于DVS的长时间监控城市公共区域的录音设置，并提供标记的DVS数据，该数据还包含了此过程中记录的环境室外影响的影响。我们还描述了用于标签生成的加工链，以及使用各种时空事件流过滤器进行的授权基准的结果。该数据集包含近7个小时的现实世界户外事件数据，并具有≈47K的景点和约当的贴标签区域。可以在http://dnt.kr.hsnr.de/DVS-OUTLAB/下载

- 直接下载：<http://dnt.kr.hsnr.de:8080/dvsoutlab/>

#### *arXiv*, Jan. 2020 A Large Scale Event-based Detection Dataset for Automotive

​	<https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/>	

​	我们介绍了第一个用于事件摄像机的非常大的检测数据集。该数据集由使用304x240 ATIS传感器获取的超过39个小时的汽车记录组成。它包含开放的道路和非常多样化的驾驶场景，包括城市，高速公路，郊区和乡村场景，以及不同的天气和照明条件。记录中包含的汽车和行人的手动边界框注释也以1到4Hz的频率提供，总共产生了超过255,000个标签。我们认为，此大小标记的数据集的可用性将有助于基于事件的视力任务（例如对象检测和分类）的重大进展。我们还期望在其他任务中的好处，例如光流，运动和跟踪的结构，例如，可以通过自我监督的学习方法利用大量数据。

#### IEEE TCYB 2023 VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows

在这项工作中，我们提出了一个大规模的可见事件基准测试（称为VisEvent），因为该任务缺乏现实和缩放的数据集。我们的数据集由820个在低照度、高速和背景杂波场景下捕获的视频对组成，并将其分为训练子集和测试子集，每个子集分别包含500个和320个视频。基于VisEvent，我们将当前的单模态跟踪器扩展到双模态版本，将事件流转换为事件图像，并构建了30多种基线方法。更重要的是，我们通过提出跨模态变换器，进一步构建了一种简单有效的跟踪算法，以实现可见数据和事件数据之间更有效的特征融合。在所提出的VisEvent数据集和两个模拟数据集（即OTB-DVS和VOT-DVS）上进行的大量实验验证了我们模型的有效性。

- <https://sites.google.com/view/viseventtrack/>
- <https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark>
- https://github.com/wangxiao5791509/RGB-DVS-SOT-Baselines

#### ICCV 2021 Object Tracking by Jointly Exploiting Frame and Event Domain

FE108数据集由DAVIS346基于事件的相机拍摄，该相机配备了346x260像素的动态视觉传感器（DVS）和有源像素传感器（APS）。它可以同时提供事件和场景的对齐灰度图像。运动目标的地面实况边界框由维康运动捕捉系统提供，该系统以高采样率（高达330Hz）和亚毫米精度捕捉运动。在捕获过程中，我们将APS的帧速率固定为20/40 FPS，将Vicon的采样率固定为240Hz，这也是捕获的APS帧和累积事件的注释频率。FE108的特点是

\>高品质标签。维康系统可以提供亚毫米精度的三维位置。

\>目标的多样性。21类（动物、车辆和日常用品）。

\>事件率的多样性。平均事件速率，单位为（0.3800]Ev/ms）。

\>现实世界的挑战。低光、高动态范围、快速运动、运动模糊等。

[[Project](https://zhangjiqing.com/dataset/)] [[added 33 videos, FE240 dataset, Baidu Cloud: password 68x3](https://pan.baidu.com/s/1gpAdfQ5Eb_GhhCDJlK3j2w)] [[DemoVideo](https://www.youtube.com/watch?v=EeMRO8XVv04&ab_channel=JiqingZhang)] [[Github](https://github.com/Jee-King/ICCV2021_Event_Frame_Tracking)] [[Dataset](https://zhangjiqing.com/dataset/contact.html)] [[Paper](https://arxiv.org/pdf/2109.09052.pdf)] [[Baiduyun](链接：https://pan.baidu.com/s/1GFfCULGbSiv7FWCKgkb8_g 提取码：AHUT)]
