### 常见激活函数 

##### Sigmoid函数

什么情况下适合使用Sigmoid？

- Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。

- 梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度

Sigmoid有哪些缺点？

- 容易造成梯度消失。我们从导函数图像中了解到sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习。

- 函数输出不是以 0 为中心的，梯度可能就会向特定方向移动，从而降低权重更新的效率

- Sigmoid 函数执行指数运算，计算机运行得较慢，比较消耗计算资源。

##### Tanh函数

什么情况下适合使用Tanh？

- tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。

**Tanh**有哪些缺点？

- 仍然存在梯度饱和的问题
- 依然进行的是指数运算

##### ReLU函数

什么情况下适合使用ReLU？

- ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和

- 由于ReLU线性、非饱和的性质，在SGD中能够快速收敛

- 计算复杂度低，不需要进行指数运算

ReLU有哪些缺点？

- 与Sigmoid一样，其输出不是以0为中心的

- Dead ReLU 问题。当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新

训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活。所以，要设置一个合适的较小的学习率，来降低这种情况的发生

##### Soft  max函数

Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大。