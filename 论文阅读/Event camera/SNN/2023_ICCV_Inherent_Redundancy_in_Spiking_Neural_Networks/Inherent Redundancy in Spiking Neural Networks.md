# Inherent Redundancy in Spiking Neural Networks

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240108121105757.png" alt="image-20240108121105757" style="zoom:80%;" />

> 尖峰神经网络(snn)是一种很有前途的节能替代传统的人工神经网络。由于人们认为网络网络是稀疏点火的，对网络网络固有冗余的分析和优化在很大程度上被忽视了，因此基于尖刺的神经形态计算在准确性和能量效率方面的潜在优势受到了干扰。**在这项工作中，我们提出并关注三个关键问题，有关固有的冗余网络。我们认为冗余是由snn的时空不变性引起的，这提高了参数利用的效率，但也带来了大量的噪声峰值。**进一步，我们分析了时空不变性对snn时空动力学和脉冲激发的影响。基于上述分析，**我们提出了一种利用snn冗余的高级空间注意(advanced Spatial Attention, ASA)模块，该模块通过一对独立的空间注意子模块自适应优化snn的膜电位分布**。这样可以准确地调节噪声的尖峰特征。实验结果表明，与现有的snnbaseline相比，该方法能显著降低脉冲触发，并具有更好的性能。我们的代码可以在https://github.com/BICLab/ASA-SNN中找到。



在本工作中，我们通过分析尖峰放电与尖峰神经元时空动态之间的关系，为理解snn的冗余提供了一个新的视角。这个分析可以通过问三个关键问题来扩展。**(i)哪些尖峰是多余的?(ii)为什么snn存在冗余?(iii)如何有效去除多余的峰值?**

为了更好地证明snn中的冗余性，我们选择基于事件的视觉任务来观察spike反应。基于事件的摄像机，如动态视觉传感器(DVS)[27]，是一类新颖的仿生视觉传感器，它只将视觉场景的亮度变化信息编码为每个像素的事件流(带有地址信息的峰值)。如图1b所示，红色和绿色的点分别表示亮度增加和减少的像素点，没有事件的灰色区域表示亮度没有变化。然而，虽然输入的信息是没有背景的人类步态，但普通SNN提取的一些spike特征集中在背景信息上。如图1c所示，噪声特征图中的尖峰神经元在背景区域触发了大量的尖峰，这些尖峰是冗余的。

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240119193640259.png" alt="image-20240119193640259" style="zoom:67%;" />
>
> 图1:(a)双脉冲输入输出、突触权重w、膜电位U(t)、阈值Vth和硬复位膜电位Vreset的脉冲神经元的时空动态。(b)事件流的例子。(c)香草SNN和ASA-SNN在膜电位分布(MPD)和穗特征方面的穗响应变化实例。脉冲特征上的每个像素值代表一个神经元的放电率。噪声尖峰特征在集中于无关紧要的背景信息(大面积红色)时，会产生大量的尖峰。ASA模块可以通过优化MPD来改变snn的尖峰模式，从而降低尖峰计数。

不幸的是，噪声特征广泛存在于时间和空间两个维度，但呈现出一些有趣的规律。我们认为，**这种现象的根本原因是由于snn的一个基本假设，即所谓的时空不变性[21]，它允许在所有时间步中的每个位置共享权值。**该假设提高了参数利用效率，同时增加了网络网络的冗余度。具体来说，通过控制事件流的输入时间窗口，我们可以清晰地观察到SNN提取的spike特征的时空变化(见图2)。**在空间维度上，存在许多类似的噪声特征，可以称之为鬼特征[14,15]**。在时间维度上，**虽然SNN提取的信息在不同的时间步长下会发生变化，但噪声峰值特征的空间位置几乎相同**。

最近有几篇文章[12,13]研究了snn在将连续的膜电位值量化为离散的尖峰时所造成的信息损失。受到这些研究成果的启发，我们将“**脉冲放电与脉冲神经元时空动态的关系**”问题转化为**膜电位分布与冗余脉冲之间的关系**。在观察到冗余与脉冲特征模式和神经元位置高度相关的基础上，我们提出了snn的高级空间注意(Advanced Spatial Attention, ASA)模块，该模块可以通过改变膜电位分布将噪声特征转换为正常或无效(没有脉冲触发)特征。我们使用各种网络结构进行了广泛的实验，以验证我们的方法在五个基于事件的数据集上的优越性。实验结果表明，ASA模块可以帮助SNN减少峰值，同时提高任务性能。例如，在DVS128 Gait-day数据集[41]上，在可以忽略不计的额外参数和计算的代价下，提出的ASA模块将基线模型的峰值计数降低了78.9%，准确性提高了5.0%。我们的贡献总结如下:

1)我们通过提出和回答三个关键问题，首次系统深入地分析了snn的固有冗余性，这三个问题对于基于spike的神经形态计算的高能量效率至关重要，但却一直被忽视。

2)我们首次将SNN的冗余性与膜电位的分布联系起来，设计了一种简单而高效的高级空间关注，帮助SNN优化膜电位的分布，从而减少冗余。

3)大量实验结果表明，所提出的ASA模块可以提高snn的性能，同时显著降低噪声峰值。这启发了我们，基于尖峰的神经形态计算的两个最重要的特性，生物启发的时空动力学和事件驱动的稀疏计算，可以自然地结合在一起，以更低的能源消耗实现更好的性能。

## 2. 相关工作

基于事件的视觉和基于尖峰的神经形态计算。分布式交换机由于具有高时间分辨率、高动态范围等独特优势，在特殊的视觉场景中具有广阔的应用前景，如高速目标跟踪[56]、低延迟交互[1]等。基于事件的视觉是snn的典型优势应用场景之一，它可以逐事件处理事件流，达到最小延迟[10]，可以平滑部署在神经形态芯片上，通过基于尖峰的事件驱动稀疏计算实现超低能量消耗[37,35,36,6]。例如，最近的边缘计算设备Speck1集成了支持snn的异步神经形态芯片和分布式交换机摄像头[10]。其峰值功率为mW级，延迟为ms级。在这项工作中，我们通过使用各种基于事件的数据集来研究snn的固有冗余性，以进一步探索它们在准确性和能效方面的诱人潜力。

注意snn。在深度学习中引入了注意力方法，并取得了巨大的成功，其动机是人类可以轻松高效地关注复杂场景中的显著视觉信息。目前比较流行的研究方向是将注意力作为辅助模块来提高神经网络的表示能力[19,44,47,26,11]。基于这一想法，Yao等人首先建议为snn使用额外的即插即用时间注意模块，以绕过一些不必要的输入时间步长。随后，研究人员对snn的多维注意模块进行了大量的研究，包括时间、空间和通道同时的注意模块[30,59,54,51,50]，其中Yao等人[51]强调，注意可以帮助snn减少脉冲触发，同时提高精度。然而，为了产生注意分数和细化膜电位，多维注意不可避免地增加了snn的额外计算负担。在本工作中，我们专门使用空间注意，这是受研究snn冗余的启发。

snn中的膜电位分布。修正MPD对于SNN的训练至关重要，因为SNN的尖峰是不连续和不可微的，因此更容易受到梯度消失或爆炸的影响。至此，研究者在SNN训练方面取得了许多进步，如归一化技术[57,46]、快捷方式设计[20,7]、参数可学性更强的扩展[8,38]、分布损失设计[13,12]等。与之前的出版物不同，我们在这里关注MPD和冗余之间的联系，这是一个在SNN社区通常被忽视的话题。

## 3.SNN冗余分析

### 3.1.SNN基本原理

首先，LIF层将执行以下集成操作，

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240119230509283.png" alt="image-20240119230509283" style="zoom:67%;" />

其中n∈{1，···，n}和t∈{1，···，t}表示层和时间步长，$U^{t,n}$表示膜电位，由空间特征$X^{t,n}$与时间信息$H^{t−1,n}$耦合产生，$X^{t,n}$可以通过卷积运算完成，

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240119230706739.png" alt="image-20240119230706739" style="zoom: 33%;" />

其中，BN(-) 和 Conv(-) 分别表示批归一化[22] 和卷积运算，$W^n$ 为权重矩阵，$S^{t,n-1}（n≠1）$为最后一层的尖峰张量，只包含 0 和 1，$X^{t,n}$ ∈ $R^{c_n×h_n×w_n}$ 。然后，尖峰神经元内部的发射机制和漏机制分别为

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240119232744481.png" alt="image-20240119232744481" style="zoom:50%;" />

and

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240119232821878.png" alt="image-20240119232821878" style="zoom:50%;" />

其中，$V_{th}$ 是阈值，用于确定输出尖峰张量 $S^{t,n}$应为尖峰还是保持为零；Hea(-) 是海维塞德阶跃函数，当 x ≥ 0 时，满足 Hea (x) = 1，否则 Hea (x) = 0；$V_{reset}$ 表示重置电位，在激活输出尖峰后设置；$β = e^{-dt \over τ} < 1$ 反映衰减因子，τ 是膜时间常数，⊗ 表示元素相乘。当 $U^{t,n}$ 中的条目大于阈值 $V_{th}$时，尖峰序列 $S^{t,n}$ 的空间输出将被激活（公式 3）。同时，$U^{t,n}$ 中的条目将重置为$V_{reset}$，时间输出 $H^{t,n}$ 应由 $X^{t,n}$ 决定，因为 $1 - S^{t,n}$ 必须为 0。否则，$U^{t,n}$ 的衰减将被用来传输$H^{t,n}$，因为$S^{t,n}$是0，这意味着没有激活的脉冲输出(公式4)。

### 3.2.冗余分析

我们首先定义了各种术语来适当地表示snn中的冗余，如下所示。

定义1。Spike Firing Rate (SFR):我们将测试集上的所有样本输入到网络中，并计算Spike的分布。在第t个时间步，我们定义一个神经元的SFR (N-SFR)为该神经元上产生峰值的样本数量与所有测试样本数量的比值。类似地，在第t个时间步，我们定义一个通道的SFR (C-SFR)或这个时间步(T-SFR)为该通道或整个网络在该时间步的所有神经元SFR值的平均值。我们定义网络平均SFR (NASFR)为T-SFR在所有时间步T上的平均值。

定义2。Spike功能。我们将测试集上的所有样本输入到网络中，**并将一个通道在第t个时间步长的平均输出定义为spike特征，每个像素的值为N-SFR。**

定义3。幽灵功能。有许多特征地图对彼此像幽灵一样相似[14,15]。我们称这些特征图为鬼特征。

定义4。尖峰模式。Spike特征显示了多种模式，各种模式提取出不同类型的信息。由于输入中没有背景信息，根据经验，我们将聚焦于背景信息的特征称为噪声模式，而将其他特征统称为正常模式。

基于这些定义，我们从空间、时间、通道和神经元四个粒度研究网络的冗余性。

观察1。在空间粒度上，脉冲响应中存在大量的重影特征。

​	在过度参数化的神经网络中，冗余是不可避免的。例如，从特征映射的角度来看，基于卷积的神经网络(cnn)中存在许多鬼特征[14,58]。snn也是如此，如图2a所示。**在同一时间步中绘制所有的spike特征，我们可以看到某些特征集中在背景信息上，有一个很大的红色区域，而另一些特征集中在步态信息上**，有一个很大的蓝色区域，两种模式中都可以看到鬼特征。

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120112604790.png" alt="image-20240120112604790" style="zoom:67%;" />
>
> 图2:snn网络的内在冗余存在于空间和时间粒度上，分别源于网络的过参数化和时空不变性。(a)不同通道在同一时间步长的峰值特征(对所有样本平均峰值张量$S^{t,n}$)。每个像素显示了尖峰神经元的放电率。像素越蓝，射击率越接近0;像素越红，射击率越接近1。在噪声特征中，背景区域较大，且几乎全部为红色，说明产生了较多的尖峰。(b)同一信道在不同时间步长的峰值特征。

观察2。**在时间粒度上，不同时间步长的T-SFR变化不大。**

考虑到每个时间步共享空间建模2D卷积的权重，进行时间建模的snn的冗余水平显著增加。**为了说明这一点，我们在图2b中给出了同一信道在不同时间步长的spike特征**。我们看到对于一个固定的通道，在不同的时间步长的特征是不同的，也就是说，人类的步态随着时间步长的增加逐渐向右移动。有趣的是，**同一频道的峰值特征——几乎全部是背景信息，或者全部是关于人类步态的信息——在不同的时间步长下本质上是相同的**。这说明时空不变性会在不同的时间步长下产生相似的脉冲特征。这也意味着**snn中的冗余与时间步长是线性连接的**。

观察3。在通道粒度上，C-SFR与该通道学习到的尖峰模式密切相关。在神经元粒度上，N-SFR与神经元的位置紧密相连。 

我们放大图1c中与各种模式相关的两个典型尖峰特征。我们看到这两个特征有本质上不同的C-SFRs。当关注琐碎的背景信息时，噪声模式的尖峰特征会触发许多尖峰。相比之下，C-SFR较低的正常模式的尖峰特征集中在一个压缩区域的显著步态信息。此外，正常特征背景区域神经元的N-SFRs几乎为零，而噪声特征背景区域神经元的N-SFRs非常高。

定义5。膜电位分布(MPD)。我们将测试集上的所有样本输入到网络中。在第n层的第c通道中，我们计算了所有神经元在第t时间步的膜电位值。我们可以用二维直方图来表示通道的膜电位分布，横轴为膜电位值，纵轴为某一窗口内的神经元数量。

观察4。膜电位分布与峰状分布高度相关。注意，为了便于分析所提方法对MPD和spike特征的影响，我们将在后面的章节5.4中详细讨论。



## 4. 方法论

动机。我们可以得出以下三个经验结论:(1)特征基本可以分为噪声和正常模式;(2)特征的质量取决于信道的MPD;(3)尖峰神经元的放电与其所处的位置有关。基于这些观察结果，我们集中优化每个通道的MPD，即执行空间注意。同时，考虑到所有特征都具有两种模式，我们利用独立的空间注意对它们分别进行优化，称为高级空间注意。

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120114214409.png" alt="image-20240120114214409" style="zoom: 50%;" />
>
> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120160642350.png" alt="image-20240120160642350" style="zoom: 67%;" />
>
> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120114256823.png" alt="image-20240120114256823" style="zoom:50%;" />
>
> 图3:ASA- snn的详细信息。ASA模块分为(b)信道分离和(c)基于组的SA两个步骤，由g1(·)、g2(·)、fS1(·)、fS2(·)四个函数组成。

方法。如图3a所示，我们分两个步骤实现ASA模块。**我们首先利用信道分离技术将所有特征根据其重要性分成两个互补的组。****然后对这两组特征执行独立的SA子模块**。假设$X^n =[···，X^{t,n}，···]∈R^{T×c_n×h_n×w_n}$是作为输入张量的中间特征映射，这两步过程可归纳为:

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120114827118.png" alt="image-20240120114827118" style="zoom: 67%;" />

**其中$M_1,M_2∈R^{T×c_n×1×1}$是只包含0和1个元素的互补掩码(分离)映射，$g_1(·)$是评估通道重要性的函数，$g_2(·)$是用于生成特征分组掩码映射的分离策略函数**，$f_{S1}(·)$和$f_{S2}(·)$是具有相同表达式的SA函数，$\overline X^n$是与$X^n$大小相同的输出特征张量，**在乘法过程中，掩模分数会相应地沿时间和通道维传播(复制)。**最后，与公式1中香草SNN的$U^{t,n}$相比，ASA-SNN层新的膜电位行为随之出现

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120115041293.png" alt="image-20240120115041293" style="zoom: 67%;" />

从经验上看，**g1(·)的设计对任务的精度、附加参数的数量和计算量都是至关重要的**。cnn中经典的通道注意模型[19,44,39,47,26]一般**通过融合特征的全局度信息(max pooling)和局部显著性信息(average pooling)来判断通道的重要性**。受这些作品的启发，我们为g1(·)设计了两种方案，一种是可学习的(ASA-1)，另一种是根据汇集的信息直接判断重要性的(ASA-2)。

如图3b所示，通过使用average-pooling和max-pooling操作来聚合时间-通道特征，从而推断出两个不同的张量$F_{avg}, F_{max}∈R^{T×c_n×1×1}$。

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120160642350.png" alt="image-20240120160642350" style="zoom: 67%;" />

在ASA-1中，我们通过

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120115202145.png" alt="image-20240120115202145" style="zoom: 50%;" />

其中，α和γ是用0.5初始化的可训练参数，σ表示sigmoid函数，$W^n_1∈R^{{T\over r} ×T}$和$W^n_2∈R^{T×{T\over r} }$是独立于每一层的可训练参数，r表示降维因子。注意，$M '，M∈R^{T×c_n×1×1}$，我们在通道维上共享$W^n_1$和$W^n_2$。

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120114256823.png" alt="image-20240120114256823" style="zoom:50%;" />

在ASA-2中，我们直接设置M = M '。将M中两个维度的第y大值组合得到$M_1$和$M_2$，分别表示重要信道指标和次重要信道指标。其中，g2(·)的伪码表示为

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120115752223.png" alt="image-20240120115752223" style="zoom:50%;" />

在得到$X^n_1 = Xn⊗M1$和$X^n_2 = X_n⊗M_2$后，我们执行单个SA模块对其进行优化。如图4所示，SA遵循[44]:

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120120622587.png" alt="image-20240120120622587" style="zoom:67%;" />

其中$AvgPool(·)，MaxPool(·)∈R^{1×1×h_n×w_n}$, $f^{3×3}$表示滤波器大小为3×3的卷积运算，$f_S(·)∈R^{1×1×h_n×w_n}$为二维空间注意力得分，设$f_{S1}(·)= f_{S2}(·)= f_S(·)$。

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120120828333.png" alt="image-20240120120828333" style="zoom: 67%;" />
>
> 图4:空间注意模块示意图。如图所示，空间注意力利用沿时间通道轴汇集的两个输出，并将它们转发到3 × 3卷积层。



## 5. 实验

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120162809504.png" alt="image-20240120162809504" style="zoom: 67%;" />
>
> 图5:ASA-SNN中的Spike特性(a)不同通道在同一时间步长的Spike特征。(b)同一信道在不同时间步长的Spike特征。

### 5.5.结果分析

asa - sns中的Spike模式。我们在第3.2节中重新检查了asa - snn中的spike响应。在空间粒度上，asa - snn中的spike模式发生了改变。**如图5a所示，几乎没有噪声特征，但出现了一些没有尖峰的空特征。在时间粒度上，时空不变性仍然存在。如图5b所示，同一信道在不同时间步长的spike特征相似。**

膜电位分布(MPD)和峰状分布。我们已经知道，snn中的冗余直接依赖于学习到的spike模式。因此，我们感兴趣的问题是“尖峰特征如何变化”，这可以帮助我们了解网络内部的动态，并启发未来的工作。在这里，我们分析了MPD与spike特征(模式)之间的关系。我们定义以下指标。

定义6。峰阈距离(PTD)。我们挑出膜电位分布中最高的三个支柱，对这些支柱的膜电位区间取平均，得到峰值区间。然后我们定义峰值到阈值的距离为峰值区间的中心点与阈值之间的差值。

观察5。利用一个通道的膜电位分布的PTD和方差可以在一定程度上衡量该通道提取的峰状特征的质量。当ptd值接近0或大于0时，说明图上大多数尖峰神经元的膜电位都在阈值的右侧。因此，大多数神经元具有较高的神经元尖峰放电率，并且直观地说，由于关键信息通常位于一个较小的区域内，通道学习到的模式是背景噪声。方差度量焦点的程度。在同一模型相同或相似的正态模式下，方差越大，所学习特征的边缘信息越清晰。

因此，如图 6 所示，我们展示了尖峰特征是如何跟随 MPD 的。具体来说，在香草 SNN 中（图 6a），模式 A 和模式 B 中的尖峰特征和 MPD 似乎是互补关系，分别对应于背景和物体区域的完美聚焦。如果 PTD 保持不变，随着方差逐渐减小，背景和物体边缘区域的信息开始模糊，如模式 C 和 D 所示。

> <img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120163437331.png" alt="image-20240120163437331" style="zoom: 50%;" /><img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120163502468.png" alt="image-20240120163502468" style="zoom: 50%;" />
>
> 图 6：插入 ASA 后，尖峰模式的膜电位分布和尖峰特征会发生变化。

然后，我们比较 vanilla 和 ASA-SNNs 的尖峰特征变化。很明显，ASA-SNNs 中所有通道的 MPD 的峰值区域都位于阈值的左侧（图 6b），即 PTD < 0。也就是说，MPD 高度紧凑，这意味着尖峰特征的边缘信息更加清晰。

结合 PTD 和方差这两个指标，我们可以快速确定 "好的 "尖峰特征的 MPD 应该是多少。例如，如 ASASNNs 的模式 B 所示，PTD 和方差值都应在一个适当的范围内，既不能太高，也不能太低。

<img src="Inherent Redundancy in Spiking Neural Networks.assets/image-20240120163830634.png" alt="image-20240120163830634" style="zoom:67%;" />

信息损失。正如文献[13, 12]所讨论的，一个好的 MPD 可以减少信息损失，而信息损失是由模拟膜电位转换为二进制尖峰时引入的量化误差（QE）引起的。与文献[13, 12]类似，我们将 QE 定义为膜电位与其相应量化尖峰值之差的平方。所提出的 ASA 模块同时优化了香草 SNN 中 MPD 的 PTD 和方差，从而大大减少了尖峰量化所造成的信息损失（见表 4）。从图 6a 和 b 的对比中可以明显看出，每个通道的 MPD 都在变薄（方差变大）。在这项工作中，方差增大意味着从特征可视化的角度来看，尖峰特征中的边缘信息更加清晰。相比之下，从 QE 的角度来看，这意味着信息损失变小了。
